from google.cloud import bigquery
from google.cloud import storage

# Initialize BigQuery client and Cloud Storage client
bigquery_client = bigquery.Client()
storage_client = storage.Client()

# Define your bucket and file names
BUCKET_NAME = 'your_bucket_name'
BIGQUERY_TABLE = 'your_project.your_dataset.your_table'
TEXT_FILE_PATH = 'path_to_column_names.txt'  # Path to your text file with column names
EXPORT_FILE_NAME = 'exported_data.csv'
TEMP_TABLE_NAME = 'your_project.your_dataset.temp_table'  # Temporary table to be created

# Step 1: Read the list of columns from the text file
def read_columns_from_txt(txt_file_path):
    with open(txt_file_path, 'r') as f:
        columns = [line.strip() for line in f.readlines()]
    return columns

# Step 2: Get the available columns in the BigQuery table
def get_available_columns_from_table(table):
    table_ref = bigquery_client.get_table(table)
    table_schema = table_ref.schema
    available_columns = [field.name for field in table_schema]
    return available_columns

# Step 3: Filter columns from the text file to include only those present in the table
def filter_existing_columns(columns, available_columns):
    return [col for col in columns if col in available_columns]

# Step 4: Create an intermediate table with selected columns
def create_intermediate_table(table, columns, temp_table):
    if not columns:
        raise ValueError("No valid columns available to create the intermediate table.")
    
    columns_str = ', '.join(columns)
    query = f"""
        CREATE OR REPLACE TABLE `{temp_table}` AS
        SELECT {columns_str} FROM `{table}`
    """
    query_job = bigquery_client.query(query)
    query_job.result()  # Wait for the query to complete
    print(f"Temporary table `{temp_table}` created successfully with columns: {columns_str}")

# Step 5: Export the intermediate table to Google Cloud Storage in CSV format with | delimiter
def export_table_to_gcs(temp_table, bucket_name, export_file_name):
    destination_uri = f"gs://{bucket_name}/{export_file_name}"
    
    extract_job = bigquery_client.extract_table(
        temp_table,
        destination_uri,
        job_config=bigquery.job.ExtractJobConfig(
            destination_format="CSV",
            field_delimiter="|",  # Use | as the delimiter
            print_header=True  # Include header in the CSV
        )
    )
    extract_job.result()  # Wait for the job to complete
    print(f"Table exported to GCS bucket: {destination_uri}")

# Step 6: Optionally, clean up the temporary table
def delete_temp_table(temp_table):
    bigquery_client.delete_table(temp_table, not_found_ok=True)
    print(f"Temporary table `{temp_table}` deleted.")

# Main function
def main():
    # Step 1: Read columns from the text file
    columns_from_file = read_columns_from_txt(TEXT_FILE_PATH)
    
    # Step 2: Get available columns from the BigQuery table schema
    available_columns_in_table = get_available_columns_from_table(BIGQUERY_TABLE)
    
    # Step 3: Filter the columns to include only those that exist in the BigQuery table
    valid_columns = filter_existing_columns(columns_from_file, available_columns_in_table)
    
    if not valid_columns:
        print("None of the columns from the text file exist in the BigQuery table.")
        return
    
    # Step 4: Create a temporary table with the valid columns
    create_intermediate_table(BIGQUERY_TABLE, valid_columns, TEMP_TABLE_NAME)
    
    # Step 5: Export the temporary table to a CSV file in GCS with | as delimiter
    export_table_to_gcs(TEMP_TABLE_NAME, BUCKET_NAME, EXPORT_FILE_NAME)
    
    # Step 6: Clean up by deleting the temporary table
    delete_temp_table(TEMP_TABLE_NAME)

if __name__ == "__main__":
    main()
