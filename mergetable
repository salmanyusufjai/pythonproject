from google.cloud import bigquery
from collections import defaultdict

# Initialize a BigQuery client
client = bigquery.Client()

# Master table information
master_table = "project.dataset.master_table"

# Your dictionary input
table_dict = [
    {'retro_date': '2023-01-01', 'output_table_name': 'project.dataset.table1_ebi'},
    {'retro_date': '2023-01-01', 'output_table_name': 'project.dataset.table2'},
    {'retro_date': '2023-01-01', 'output_table_name': 'project.dataset.table3'},
    {'retro_date': '2023-02-01', 'output_table_name': 'project.dataset.table4_ebi'},
    {'retro_date': '2023-02-01', 'output_table_name': 'project.dataset.table5'}
]

# Step 1: Group tables by retro_date
grouped_tables = defaultdict(list)
for entry in table_dict:
    retro_date = entry['retro_date']
    output_table_name = entry['output_table_name']
    grouped_tables[retro_date].append(output_table_name)

# Helper function to extract alias name from the full table name
def get_alias_name(full_table_name):
    # Extract alias (table name without project and dataset, assuming format is project.dataset.tablename)
    return full_table_name.split('.')[-1]  # Returns "table2" from "project.dataset.table2"

# Function to process each group
def process_group(rundate, tables):
    # Identify the table that contains 'ebi' in its name
    ebi_table = None
    other_tables = []
    
    for table in tables:
        if "ebi" in table:
            ebi_table = table
        else:
            other_tables.append(table)
    
    if not ebi_table:
        raise ValueError(f"No 'ebi' table found for rundate {rundate}")
    
    # Step 2: Build the SQL query dynamically based on columns
    # Use alias for ebi_table
    ebi_table_alias = get_alias_name(ebi_table)

    query = f"""
    -- Get companyid from master table
    WITH master AS (
        SELECT companyid FROM `{master_table}`
    ),
    -- Get data from ebi table excluding companyid
    ebi_data AS (
        SELECT * EXCEPT(companyid)
        FROM `{ebi_table}` AS {ebi_table_alias}
        WHERE companyid IN (SELECT companyid FROM master)
    )
    """
    
    # Add other tables dynamically with their aliases and columns
    for table in other_tables:
        table_alias = get_alias_name(table)
        query += f""",
        {table_alias}_data AS (
            SELECT * EXCEPT(companyid)
            FROM `{table}` AS {table_alias}
            WHERE companyid IN (SELECT companyid FROM master)
        )
        """
    
    # Final SELECT to combine data
    query += f"""
    SELECT master.companyid, ebi_data.*, 
    """
    query += ", ".join([f"{get_alias_name(table)}_data.*" for table in other_tables])  # Add other table data columns
    query += f"""
    FROM master
    LEFT JOIN ebi_data ON master.companyid = ebi_data.companyid
    """
    
    for table in other_tables:
        table_alias = get_alias_name(table)
        query += f"LEFT JOIN {table_alias}_data ON master.companyid = {table_alias}_data.companyid\n"

    # Step 4: Execute the query and save the result into a new table
    new_table_name = f"project.dataset.new_table_{rundate.replace('-', '_')}"
    job_config = bigquery.QueryJobConfig(destination=new_table_name)
    
    query_job = client.query(query, job_config=job_config)
    query_job.result()  # Wait for the query to complete
    
    print(f"New table created: {new_table_name}")

# Step 5: Process each group by retro_date
for rundate, tables in grouped_tables.items():
    process_group(rundate, tables)
