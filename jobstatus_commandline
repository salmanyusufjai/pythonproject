if __name__ == "__main__":
    job_id = start_dataflow_job()
    if job_id:
        wait_for_job_completion(job_id)
import subprocess
import re
import time

def start_dataflow_job():
    command = [
        "gcloud", "dataflow", "jobs", "run", "my-job",
        "--gcs-location=gs://your-bucket/templates/your-template",
        "--parameters", f"input_query={self.input_query},output_table={self.output_table}",
        "--region=your-region",
        "--project=your-project-id"
    ]

    # Start the job
    result = subprocess.run(command, capture_output=True, text=True)
    
    if result.returncode != 0:
        print("Failed to start Dataflow job.")
        print("STDERR:", result.stderr)
        return None

    # Extract the job ID
    job_id_pattern = re.compile(r'Created job with ID: (\S+)')
    match = job_id_pattern.search(result.stdout)

    if match:
        return match.group(1).strip()
    else:
        print("Job ID not found.")
        return None




def check_job_status(job_id):
    command = [
        "gcloud", "dataflow", "jobs", "describe", job_id,
        "--format=value(currentState)",
        "--project=your-project-id",
        "--region=your-region"
    ]
    
    try:
        result = subprocess.run(command, capture_output=True, text=True)
        if result.returncode == 0:
            status = result.stdout.strip()
            return status
        else:
            print(f"Error checking job status: {result.stderr}")
            return None
    except Exception as e:
        print(f"Exception occurred: {e}")
        return None

def wait_for_job_completion(job_id):
    while True:
        status = check_job_status(job_id)
        if status in ['JOB_STATE_DONE', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED']:
            print(f"Job {job_id} has finished with status: {status}")
            break
        else:
            print(f"Job {job_id} is still running. Waiting...")
            time.sleep(30)  # Wait before polling again
