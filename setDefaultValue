import pandas as pd
from google.cloud import bigquery
from pandas_gbq import to_gbq

# Initialize BigQuery client
client = bigquery.Client()

# Define your project and dataset details
project_id = 'your_project_id'
dataset_id = 'your_dataset'
table_id = 'your_table'
destination_table = 'your_new_table'

# Define chunk size (adjust based on available memory)
chunk_size = 10000  # Adjust this number based on your system's memory

# Query to fetch only records where EntityKey is NULL
query = f"""
    SELECT *
    FROM `{project_id}.{dataset_id}.{table_id}`
    WHERE EntityKey IS NULL
"""

# Use BigQuery's pagination feature to fetch data in chunks
iterator = client.query(query).to_dataframe_iterable(max_results=chunk_size)

# Process each chunk
for df_chunk in iterator:
    # List of columns to exclude from modification
    exclude_columns = ['EntityKey', 'CompanyNumber']

    # Get all columns except the excluded ones
    columns_to_modify = [col for col in df_chunk.columns if col not in exclude_columns]

    # Convert all selected columns to string and set to '_'
    df_chunk[columns_to_modify] = df_chunk[columns_to_modify].astype(str)
    df_chunk[columns_to_modify] = '_'

    # Append the processed chunk to a new BigQuery table
    to_gbq(df_chunk, f'{dataset_id}.{destination_table}', project_id=project_id, if_exists='append')

print("Processing completed and data has been written to BigQuery.")
