from google.cloud import bigquery
from google.cloud import storage

# Initialize BigQuery and Cloud Storage clients
bigquery_client = bigquery.Client()
storage_client = storage.Client()

# Set project, dataset, and table details
project_id = 'your_project_id'
dataset_id = 'your_dataset_id'
old_table_name = 'old_table_name'
new_table_name = 'new_table_name'
bucket_name = 'your_bucket_name'
csv_file_name = 'your_file_name.csv'

# Step 1: Define the column renaming dictionary
# The key is the old column name, and the value is the new column name
column_rename_dict = {
    "old_column_name1": "new_column_name1",
    "old_column_name2": "new_column_name2",
    # Add as many columns as you need
}

# Step 2: Dynamically build the SELECT query with the renamed columns
def build_dynamic_query():
    # Fetch the schema of the old table
    table = bigquery_client.get_table(f"{project_id}.{dataset_id}.{old_table_name}")
    
    # Iterate through the schema to build the dynamic select query
    select_columns = []
    for field in table.schema:
        if field.name in column_rename_dict:
            # Rename column
            select_columns.append(f"{field.name} AS {column_rename_dict[field.name]}")
        else:
            # Keep column as is
            select_columns.append(field.name)
    
    # Create the query string dynamically
    query = f"""
    CREATE OR REPLACE TABLE `{project_id}.{dataset_id}.{new_table_name}` AS
    SELECT {', '.join(select_columns)}
    FROM `{project_id}.{dataset_id}.{old_table_name}`;
    """
    
    return query

# Step 3: Rename columns dynamically using the query
def rename_columns_dynamically():
    query = build_dynamic_query()
    query_job = bigquery_client.query(query)
    query_job.result()  # Wait for the query to finish
    print(f"Table {new_table_name} created with renamed columns.")

# Step 4: Export the renamed table to Google Cloud Storage as pipe-separated CSV
def export_table_to_gcs():
    destination_uri = f"gs://{bucket_name}/{csv_file_name}"
    
    # Configure extract job
    extract_job = bigquery_client.extract_table(
        f"{project_id}.{dataset_id}.{new_table_name}",
        destination_uri,
        location="US",  # Specify the region where your BigQuery dataset is located
        job_config=bigquery.job.ExtractJobConfig(
            destination_format="CSV",
            field_delimiter="|",  # Set pipe delimiter
            print_header=True,  # Include the header in the export
        ),
    )
    
    extract_job.result()  # Wait for the job to complete
    print(f"Data successfully exported to {destination_uri} as a pipe-separated CSV.")

# Step 5: Run both steps
rename_columns_dynamically()
export_table_to_gcs()
